{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9db06ff3",
   "metadata": {},
   "source": [
    "## Code implementation\n",
    "### Version: 0 [17-08-2024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0755551",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "# Define the Expert Layer\n",
    "class Expert(nn.Module):\n",
    "    '''\n",
    "    Class Name: Expert\n",
    "    Description: \n",
    "        The `Expert` class represents an individual expert within the Mixture-of-Experts (MoE) model. \n",
    "        Each expert is a simple neural network layer that processes input data and applies a ReLU activation function. \n",
    "        The experts in the MoE model are responsible for learning different aspects of the input features.\n",
    "    \n",
    "    Attributes:\n",
    "        fc (torch.nn.Linear): A fully connected (linear) layer that takes the input dimension and outputs a transformed feature.\n",
    "\n",
    "    Methods:\n",
    "        forward(x): \n",
    "            Description: Applies a linear transformation followed by a ReLU activation function to the input tensor `x`.\n",
    "            Input: x (torch.Tensor): The input data tensor.\n",
    "            Output: torch.Tensor: The transformed data tensor after applying the linear layer and ReLU.\n",
    "    '''\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Expert, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu(self.fc(x))\n",
    "\n",
    "# Define the Gating Network\n",
    "class GatingNetwork(nn.Module):\n",
    "    '''\n",
    "    Class Name: GatingNetwork\n",
    "    Description: \n",
    "        The `GatingNetwork` class is a critical component of the Mixture-of-Experts (MoE) model. \n",
    "        It acts as a softmax classifier that determines the weighting of each expert's output based on the input data. \n",
    "        This class enables the MoE model to activate different experts dynamically depending on the input.\n",
    "\n",
    "    Attributes:\n",
    "        fc (torch.nn.Linear): A fully connected (linear) layer that produces the logits used to determine expert weights.\n",
    "\n",
    "    Methods:\n",
    "        forward(x): \n",
    "            Description: Passes the input tensor `x` through a linear layer and applies a softmax function to produce a probability distribution over the experts.\n",
    "            Input: x (torch.Tensor): The input data tensor.\n",
    "            Output: torch.Tensor: A tensor representing the probabilities of selecting each expert.\n",
    "    '''\n",
    "    def __init__(self, input_dim, num_experts):\n",
    "        super(GatingNetwork, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, num_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.softmax(self.fc(x), dim=-1)\n",
    "\n",
    "# Define the Mixtures-of-Experts Model\n",
    "class MoEModel(nn.Module):\n",
    "    '''\n",
    "    Class Name: MoEModel\n",
    "    Description: \n",
    "        The `MoEModel` class represents the overall Mixture-of-Experts (MoE) model. \n",
    "        It combines the outputs of several `Expert` instances based on the gating network's decision.\n",
    "        The model processes input data by passing it through the gating network, \n",
    "        which assigns weights to the outputs of each expert. The weighted outputs are then summed \n",
    "        to form the final output, which is passed through a final linear layer for classification.\n",
    "\n",
    "    Attributes:\n",
    "        experts (torch.nn.ModuleList): A list of `Expert` instances that process the input data.\n",
    "        gating_network (GatingNetwork): The network that determines the weighting of each expert's output.\n",
    "        fc_out (torch.nn.Linear): A final fully connected layer that produces the output prediction.\n",
    "\n",
    "    Methods:\n",
    "        forward(x): \n",
    "            Description: Passes the input tensor `x` through the gating network to obtain expert weights,\n",
    "                         then applies each expert to `x`, combines their outputs based on the gating weights, \n",
    "                         and finally applies a fully connected layer to produce the output.\n",
    "            Input: x (torch.Tensor): The input data tensor.\n",
    "            Output: torch.Tensor: The final output tensor after combining the expert outputs.\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_experts):\n",
    "        super(MoEModel, self).__init__()\n",
    "        self.experts = nn.ModuleList([Expert(input_dim, hidden_dim) for _ in range(num_experts)])\n",
    "        self.gating_network = GatingNetwork(input_dim, num_experts)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate_outputs = self.gating_network(x)\n",
    "        expert_outputs = torch.stack([expert(x) for expert in self.experts], dim=2)\n",
    "        weighted_expert_output = torch.sum(expert_outputs * gate_outputs.unsqueeze(2), dim=1)\n",
    "        output = self.fc_out(weighted_expert_output)\n",
    "        return output\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = 784  # For MNIST dataset (28x28 images)\n",
    "hidden_dim = 128\n",
    "output_dim = 10  # Number of classes in MNIST\n",
    "num_experts_list = [2, 4, 6, 8]  # List of different numbers of experts to test\n",
    "\n",
    "# Load the MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "\n",
    "# Set up K-Fold Cross Validation\n",
    "k_folds = 5\n",
    "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
    "\n",
    "# Cross-validation loop\n",
    "best_num_experts = None\n",
    "best_loss = float('inf')\n",
    "\n",
    "for num_experts in num_experts_list:\n",
    "    fold_losses = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n",
    "        print(f'FOLD {fold + 1}/{k_folds} - Num Experts: {num_experts}')\n",
    "\n",
    "        # Sample elements randomly from a given list of indices\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "        val_subsampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "\n",
    "        # Define data loaders for training and validation\n",
    "        train_loader = DataLoader(dataset, batch_size=64, sampler=train_subsampler)\n",
    "        val_loader = DataLoader(dataset, batch_size=64, sampler=val_subsampler)\n",
    "\n",
    "        # Instantiate the model\n",
    "        model = MoEModel(input_dim, hidden_dim, output_dim, num_experts)\n",
    "        \n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "        # Training loop\n",
    "        num_epochs = 5\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            for images, labels in train_loader:\n",
    "                images = images.view(-1, 28 * 28)  # Flatten the images\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Backward pass and optimization\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images = images.view(-1, 28 * 28)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        fold_losses.append(val_loss / len(val_loader))\n",
    "        print(f'Fold {fold + 1} - Validation Loss: {val_loss / len(val_loader):.4f}')\n",
    "\n",
    "    # Calculate average loss over all folds\n",
    "    avg_loss = np.mean(fold_losses)\n",
    "    print(f'Average Validation Loss for {num_experts} experts: {avg_loss:.4f}')\n",
    "\n",
    "    # Update best number of experts\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        best_num_experts = num_experts\n",
    "\n",
    "print(f'Best number of experts: {best_num_experts} with average validation loss: {best_loss:.4f}')\n",
    "\n",
    "# Final training with the best number of experts\n",
    "model = MoEModel(input_dim, hidden_dim, output_dim, best_num_experts)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "train_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    for images, labels in train_loader:\n",
    "        images = images.view(-1, 28 * 28)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\"Final training completed with the best number of experts.\")\n",
    "\n",
    "# Save the final model\n",
    "torch.save(model.state_dict(), 'moe_best_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35561b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae86008d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
